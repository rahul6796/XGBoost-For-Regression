{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Maths Details For Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases , we build the trees using $Similarity$ $Scores$ and then calculate the $Output$ $Values$ for the leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will derive the equations for the $Similarity$ $Score$ and $Output$ $Values$ and show you how the only difference between $Regression$ and $Classification$ is the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Similarity$ $Score$ and $Output$ $Value$ for $Regression$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score = \\frac{Sum Of Residuals, Squared}{NumberOf Residuals + {\\lambda}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and \n",
    "\\begin{equation}\n",
    "Output Value = \\frac{SumOfResiduals}{Number Of Residuals + {\\lambda}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the examples manageable , we will start with this simple $Training$ $Datasets$ for $Regression$.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data which are taking as an examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for $Regression$ we are using $Drug$ $Dosage$ on the $x-axis$, to predict $Drug$ $Effectiveness$ on the $y-axis$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Both $Regression$ and $Classification$ we already know that $XGBoost$ starts with an initial  prediction that is usually $0.5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and in both cases we can represent this prediction with a thick black line at $0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the $Residuals$, the differences between the observed and $Predicted$ values ,show us how good the initial prediction is.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in regular , unextreme $Gradient$ $Boost$ , we can quantify how good the prediction is with a $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i}) = \\frac{1}{2}(y_{i} - p_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $Gradient$ $Boost$ $Regression$ $Details$, we learned how to use this $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To review $y_{i}$ stands for the $y-axis$ value from one of the observed values $y_{1}$,$y_{2}$,$y_{3}$.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $p_{i}$ stands for a prediction , $p_{1}$,$p_{2}$and$p_{3}$ that corresponds to one of the observation $y_{1}$,$y_{2}$and$y_{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example , if are applied this $Loss$ $Function$ to the initial prediction..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we add on single term for each observation , so $n$ = $3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{3}L(y_{i},p_{i}) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{3}L(y_{i},p_{i}) = \\frac{1}{2}(y_{1} - p_{1})^{2} +\\frac{1}{2}(y_{2} - p_{2})^{2}+\\frac{1}{2}(y_{3} - p_{3})^{2} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first , second and third trems in the summation corresponds to the first , second and third observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we just plug in number in this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{3}L(y_{i},p_{i}) = \\frac{1}{2}(-10 - 0.5)^{2} +\\frac{1}{2}(7 - 0.5)^{2}+\\frac{1}{2}(8 - 0.5)^{2} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{3}L(y_{i},p_{i}) = 104.4\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we can apply the $Loss$ $Function$ to new predictions and compare the results to this one to determine if our prediction are improving or not.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot for  again data but as an n example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note:-$ if we had $n$ observations, then we add $n$ terms in our $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{3}L(y_{i},p_{i}) = \\frac{1}{2}(y_{1} - p_{1})^{2} +\\frac{1}{2}(y_{2} - p_{2})^{2}+ .....+ \\frac{1}{2}(y_{n}- p_{n})^{2} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have one $Loss$ $Function$ for $Regression$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$XGBoost$  uses those  $Loss$ $Function$ to build trees by minimizing this $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note-:$ The equation in  the original manuscript  for $XGBoost$ contains an extra trems that i am omitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p_{i}) + {\\gamma}*T + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trem , ${\\gamma}*T$ , where $T$ is the number of $Terminal$ nodes or, leaves in tree, and {\\gamma}(gamma) is a user definable penalty, is meant to encourage pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I say that it encourage pruning because as we saw in $XGBoost$ can prune even when ${\\gamma}$ = $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am omitting this term because as we saw in $XGBoost$ for $Regression$ and $Clssification$ pruning takes place ofter the full tree in built, and it plays no role in deriving the $Optimal$ $Output$ $Values$ or $Similarity$ $Scores$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's talk about this equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p_{i}) + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part is the $Loss$ $Function$ which we just talked about and the second part consist of $Regularization$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find an $Output$ $Value$ $(O_{value})$ for the leaf that minimizes the whole equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in a way that is very similar to $Ridge$ $Regression$ we square to $Output$ $Value$ from the new tree, and scale it with ${\\lambda}$(lambda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on  I will show you that just like $Ridge$ $Regression$ , if ${\\lambda}$ > $0$ then we will shrink the $(O_{value})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note-:$ Because we optimizing the output value from the first tree, we can replace the prediction $p_{i}$, with the initial prediction $p^{0}$ , plus the $Output$ $Value$ $(O_{value})$ from the new tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value}) + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand all of the terms in this equation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw the first tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first node as initila prediction and second which we are going to build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use if to build the first tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the three data point for the regression ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by putting  all of the $Residuals$ into a single leaf, and now we need to find an $Output$ $Value$ $(O_{value})$ for this leaf that will minimize this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we dive  into the math let's simplify thing by setting ${\\lambda}$(lambda) equal to $0$, and that means removing the $Regularization$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's plug in different $Output$ $Value$ $(O_{value})$ for the leaf and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example if we set the $(O_{value})$ = $0$, then we are left with the loss fucntion for the inital prediction , $0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value}) + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},0.5 + 0)   = \\frac{1}{2}(y_{i}-p_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note:-$ we already calculated the $Loss$ $Function$ for the initial prediction when we demonstrated the $Regression$ $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we got .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},0.5 + 0)   = \\frac{1}{2}(y_{i}-p_{i})^{2} = 104.4\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the result on this graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the result on thus graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $x-axis$ on this graph represents different value for $(O_{value})$,and $y-axis$ represent the sum of the $Loss$ $Function$ for each observed value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens if we set the $(O_{value})$ = $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the $Output$ $Value$ for the leaf is $-1$, then the new prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "0.5 - 1 = -0.5\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the original data for perfoming this task.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $-0.5$ corresponds to this new $thick$ $black$ $line$, and the shrink the $Residual$ for $y_{1}$ , but its make $Residuals for both $y_{2}$ and  $y_{3}$  larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},0.5 + -1)   = \\frac{1}{2}(y_{i}-p_{i})^{2} = 109.4\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the updated graph of this point .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the point on this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that $-1$ is a worse worse choice for the $Output$ $Value$ than $0$ , because it has larger $Total$ $Loss$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast if we set the $O_{value}$  to postive $1$,then the new prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "0.5 + 1 = 1.5\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that make the $Residual$ for $y_{1}$  larger ,but the $Residuals$ for both $y_{2}$and $y_{3}$ are smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},0.5 + 1)   = \\frac{1}{2}(y_{i}-p_{i})^{2} = 102.4\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the new updated the graph.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that we have the lowest total so far..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and thus $+1$ is the best $Output$ $Value$ we have picked so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note:- $ we can keep plugging in numbers for the $O_{value}$ to see what we get.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drwa the doted value of the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can just plot the function as curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curve shows us that when ${\\lambda}$(lambdas) is $0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the optimal $O_{value}$ is at the bottom of the parabola where the ferivative is $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we increase ${\\lambda}$(lambda) $4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then the lowest point in the parabola shifts even closer to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we increase ${\\lambda}$ (lambda) to $40$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then  the lowest point in the parabola shifts even closer to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words the more emphasis we give the $Regularization$ penalty be increasing  ${\\lambda}$ (lambda) the optimal $O_{value}$ gets closer to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is exactly what $Regularization$ is supposed to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one last things before we solve this equation for the optimal $Output$ $Value$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember that when regular unextreme $Gradient$ $Boost$ found the optimal $Output$ $Value$ for a leaf , it solved an equation, very similar to $Regression$ without the $Regularization$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value})  + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unextreme $Gradient$ $Boost$ used two techniques to solve this equation. One for $Regression$,because the math was easy , and  a different one for $Classification$ , because  the math was not easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast $XGBoost$ uses the $Second$ $Order$ $Taylor$ $Approximation$ for both $Regression$ and $Classification$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i} + O_{value}) \\approx L(y,p_{i}) +[\\frac{\\partial }{\\partial p_{i}} L(y,p_{i})]O_{value} + \\frac{1}{2}[\\frac{\\partial^{2} }{\\partial p_{i}^{2}}L(y,p_{i})]O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $Loss$ $Function$ that includes the $O_{value}$, can be approximated by this mess of sums and derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genius of a Taylor $Approximation $  is that it's made of relatively simple parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y,p_{i}) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the $Loss$ $Funtion$ for the previous prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "[\\frac{\\partial }{\\partial p_{i}} L(y,p_{i})]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first derivative of the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "[\\frac{\\partial^{2} }{\\partial p_{i}^{2}}L(y,p_{i})]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is the second derivative of the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note:-$ Since the first derivative of a loss function is related to something called a $Gradient$, $XGBoost$ used ${(g)}$ to represent the derivative of the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since the second derivative of a loss function is related to something called a $(Hessian)$  , $XGBoost$ uses $(h)$ to represent to second derivative of the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i} + O_{value}) \\approx L(y,p_{i}) +g*O_{value} + \\frac{1}{2}*h*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's expand the summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = L(y_{1},p_{1}^{0}+O_{value}) +L(y_{2},p_{2}^{0}+O_{value})  + .....+L(y_{n},p_{n}^{0}+O_{value})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the $Regularized$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = L(y_{1},p_{1}^{0}+O_{value}) +L(y_{2},p_{2}^{0}+O_{value})  + .....+L(y_{n},p_{n}^{0}+O_{value}) + \\frac{1}{2}{\\lambda}*O_{value}^{2} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and  plug in the $Second$ $Order$ $Taylor$  $Approximation$ for each $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= L(y_{1},p_{1}^{0}) +g_{1}*O_{value} + \\frac{1}{2}*h_{1}*O_{value}^{2}+L(y_{2},p_{2}^{0}) +g_{2}*O_{value} + \\frac{1}{2}*h_{2}*O_{value}^{2} +...+L(y_{n},p_{n}^{0}) +g_{n}*O_{value} + \\frac{1}{2}*h_{n}*O_{value}^{2} + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on ,let's remember that we want to find an $Output$ $Value$ $(O_{value})$ that minimize the $Loss$ $Function$ plus the $Regularization$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value})  + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and that all we have done so far is approximate this above equation we want to minimize with a $Second$ $Order$ $Taylor$ $Polynomial$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's worth noting that these terms do not contain the $Output$ $Value$ $(O_{value})$, and  that means they have no effect on the optimal $Output$ $Value$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = L(y_{1},p_{1}^{0})+L(y_{2},p_{2}^{0})+....+L(y_{n},p_{n}^{0})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can omit them from the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= g_{1}*O_{value} + \\frac{1}{2}*h_{1}*O_{value}^{2}+g_{2}*O_{value} + \\frac{1}{2}*h_{2}*O_{value}^{2} +...+g_{n}*O_{value} + \\frac{1}{2}*h_{n}*O_{value}^{2} + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains are terms associated with the $O_{value}$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's combine all of the unsquared $O_{value}$ terms into single term, and combine all of he squared $O_{value}$ terms into a single term.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= (g_{1} + g_{2}+...+g_{n})*O_{value} + \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do what we usually do when we want a value that minimize's a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this above statement we follow some steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$ - Take the derivative with respect to the $Output$ $Value$ $(O_{value})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$-Set the derivative equal to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$- Solve for the $Output$ $Value$ $O_{value}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the first term , with respect to the $O_{value}$ , is just the sum of the g's..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial }{\\partial O_{value}}(g_{1} + g_{2}+...+g_{n})*O_{value}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " = (g_{1} + g_{2}+...+g_{n})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take the derivative of the second term with respect  to the $O_{value}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial }{\\partial O_{value}}\\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaving us the sum of the h's and ${\\lambda}$(lambda) times the $O_{value}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= (h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the derivative equal to $0$,and  solve for the $O_{value}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "(g_{1} + g_{2}+...+g_{n}) +(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value} = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{-(g_{1} + g_{2}+...+g_{n})}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##draw the digram for that node which are calculating the output values.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we need to plug in the $Gradients$ the $(g's)$, and the $Hessian$ the $(h's)$ for the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we using $XGBoost$ for $Regression$. Then this is the most commonly used $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i}) = \\frac{1}{2}(y_{i}-p_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first derivative of this $Loss$ $Function$,the $Gradient$ ,$g_{i}$ with respect to the predicted value ,$p_{i}$ is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=\\frac{\\partial}{\\partial p_{i}}L(y_{i},p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=\\frac{\\partial}{\\partial p_{i}}\\frac{1}{2}(y_{i} - p_{i})^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g_{i} = -(y_{i} - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $g_{i}$ is the negative $Residual$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means we plug in a negative $Residual$ for each $g_{i}$ in the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{-(g_{1} + g_{2}+...+g_{n})}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{-(-(y_{1} - p_{1}) - -(y_{2} - p_{2}) +.....+ -(y_{n} - p_{n}))}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{((y_{1} - p_{1}) +(y_{2} - p_{2}) +.....+ (y_{n} - p_{n}))}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the whole numerator is just the sum of the $Residuals$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{Sum Of Residuals}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to figure out what the $h's$ are in the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second derivative , of the $Loss$ $Function$ , the $Hessian$,$h_{i}$ with respect to the predicted value,p_{i} is.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i}) = \\frac{1}{2}(y_{i}-p_{i})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i}=\\frac{\\partial^{2}}{\\partial p_{i}^{2}}\\frac{1}{2}(y_{i} - p_{i})^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i}= \\frac{\\partial }{\\partial p_{i}}[\\frac{\\partial }{\\partial p_{i}}\\frac{1}{2}(y_{i} - p_{i})^2]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= \\frac{\\partial }{\\partial p_{i}}-(y_{i} - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i} = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that means we replace all $n$, $h's$ in the denominator with the number $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{Sum Of Residuals}{(1+1 +....+1 +{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words the denominator  is the number of $Residual$ plus ${\\lambda}$ (lambda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{Sum Of Residuals}{(Number Of Residuals+{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, When we are using $XGBoost$ for $Regression$, this is the specific formula for the $Output$ $Value$ for a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
